Attention Alignment and Flexible Positional Embeddings
Improve Transformer Length Extrapolation

Ta-Chung Chi
Carnegie Mellon University
tachungc@andrew.cmu.edu

Ting-Han Fan
Independent Researcher
tinghanf@alumni.princeton.edu

Alexander I. Rudnicky
Carnegie Mellon University
air@cs.cmu.edu

3
2
0
2

v
o
N
1

]
L
C
.
s
c
[

1
v
4
8
6
0
0
.
1
1
3
2
:
v
i
X
r
a

Abstract

An ideal length-extrapolatable Transformer lan-
guage model can handle sequences longer than
the training length without any long sequence
fine-tuning. Such long-context utilization ca-
pability highly relies on a flexible positional
embedding design. Upon investigating the flex-
ibility of existing large pre-trained Transformer
language models, we find that the T5 family
deserves a closer look, as its positional embed-
dings capture rich and flexible attention pat-
terns. However, T5 suffers from the dispersed
attention issue: the longer the input sequence,
the flatter the attention distribution. To alleviate
the issue, we propose two attention alignment
strategies via temperature scaling. Our findings
improve the long-context utilization capabil-
ity of T5 on language modeling, retrieval, and
multi-document question answering without
any fine-tuning, suggesting that a flexible po-
sitional embedding design and attention align-
ment go a long way toward Transformer length
extrapolation.1

1

Introduction

Pre-training large Transformer language models
on long sequences is inherently expensive due to
self-attention’s quadratic complexity w.r.t the in-
put sequence length (Vaswani et al., 2017). Even
with the help of memory-efficient attention (Rabe
and Staats, 2021; Dao et al., 2022), the maximum
supported input length of current open-source pre-
trained Transformer language models is still capped
at 4,096 tokens (Touvron et al., 2023), limiting their
efficacy in handling long-context tasks.

One notable research topic aiming to lift
the input length restriction is Length Extrapo-
lation (Press et al., 2022).
Ideally, a length-
is
extrapolatable Transformer language model
trained on short sequences and can perform equally

1https://github.com/chijames/

Retrieval Tasks
Line

Topic

Passkey

512
0.28
3.47

15k
0.12
6.63

512
0.27
3.47

15k
0.11
7.04

512
0.32
3.09

15k
0.24
5.97

Criteria

Pmax
H

Table 1: The Dispersed Attention Issue of Flan-T5-
XL Encoder. Pmax is the averaged maximum proba-
bility and H is the averaged entropy. After increasing
the sequence length from 512 to 15k, we observe larger
entropy and smaller maximum probability, implying a
flatter self-attention distribution.

well on longer ones without any further fine-tuning.
This is made possible with carefully designed po-
sitional embeddings (Press et al., 2022; Chi et al.,
2022, 2023). Unfortunately, existing approaches
are tailored for natural language modeling, a task
known to have strong recency bias, and they of-
ten do not perform well on other seemingly simple
tasks such as passkey, topic, and line retrieval (Mo-
htashami and Jaggi, 2023; Li et al., 2023).

To circumvent the recency bias, we sift through
the positional embeddings of existing open-source
large pre-trained Transformer language models in
Table 2 to find a flexible design, and the T5 fam-
ily (Raffel et al., 2020) comes to our attention. As
visualized in Figure 1, the flexibility of T5’s posi-
tional embeddings allows it to encourage recency
bias on one head and discourage that on another
head. However, there is no free lunch: T5 suffers
from the dispersed attention issue as shown in Ta-
ble 1. In a nutshell, the attention distributions of
long input sequences tend to be flatter than those
of short input sequences. As a remedy, we propose
two fine-tuning-free attention alignment strategies
via Softmax temperature scaling (Yao et al., 2021;
Su, 2021) to mitigate the dispersed attention is-
sue: maximum probability (Pmax) or entropy (H)
alignment.

Attention-Alignment-Transformer-Length-Extrapolation

We validate the effectiveness of our alignment

 
 
 
 
 
 
Models T5 (2020) OPT (2022) ChatGLM (2022) LLaMA (2023) Falcon (2023) Pythia (2023) XGen (2023) BLOOM (2022) MPT (2023)

PE.

Learned
Learned
Relative Absolute

Rotary
Relative

Rotary
Relative

Rotary
Relative

Rotary
Relative

Rotary
Relative

ALiBi
Relative

ALiBi
Relative

Table 2: Open-source Transformer language models and their positional embeddings. T5 is the only model
equipped with learnable relative positional embeddings, which enable its long-context utilization capability.

strategies on five tasks including language model-
ing, retrieval, and multi-document question answer-
ing. We also provide a theoretical analysis of how
our alignment strategies work by under the hood
investigating the relation between the Softmax tem-
perature and data distribution.

2 Related Work

Transformer
Embeddings
Positional
Transformer-based models rely on positional
embeddings to encode positional
information.
We summarize open-source large pre-trained
Transformer language models and their positional
embeddings in Table 2. The relative variants
are widely adopted due to their better empirical
performance (Su et al., 2021) and possible
length-extrapolatable capability (Press et al., 2022).
In this work, we put special focus on the T5
positional embeddings due to their flexibility as
shown in Figure 1.

Transformer Length Extrapolation Existing
research on Transformer length extrapolation is
mostly done on the task of natural language mod-
eling (Press et al., 2022; Chi et al., 2022, 2023).
Unfortunately, the reported positive results do not
carry over to the task of long-context retrieval (Mo-
htashami and Jaggi, 2023; Li et al., 2023). This
contrastive observation can be explained by mod-
els’ short empirical receptive field (Chi et al., 2023).
In short, the strong decaying prior of positional em-
beddings prevents models from accessing faraway
tokens that are necessary for retrieval tasks. In this
work, we improve the flexible positional embed-
dings of T5 to get around this limitation.

Transformer Position Interpolation Instead of
performing direct length extrapolation, another line
of research conducts model fine-tuning on long
input sequences (Chen et al., 2023), where the
main focus is to identify the most efficient fine-
tuning scheme that improves long-context utiliza-
tion. Positive results have been reported on retrieval
tasks (Li et al., 2023). However, we argue that
fine-tuning incurs additional costs since it needs

1) GPU resources to perform long sequence fine-
tuning with large models and 2) a pre-defined target
sequence length, which still imposes a sequence
length upper limit. Our proposed methods can cir-
cumvent these two limitations.

with

Tasks

Retrieval
Transformers
Transformer-based approaches often consist
of a retriever and a reader to overcome the context
length restriction (Guu et al., 2020; Lewis et al.,
2020; Izacard and Grave, 2021; Borgeaud et al.,
2022). The retriever retrieves relevant text snippets
from a huge database and the reader digests the
retrieved information to generate the correct
output. Our proposed attention alignment strategy
can be used to significantly increase the input
sequence length of the reader, thereby allowing
more retrieved information to participate in
the decision process. For small-scale retrieval
problems, our methods even obviates the need for
context segmentation and an external key-value
store used in prior work (Mohtashami and Jaggi,
2023), serving as a more elegant approach.

Softmax Temperature Scaling To increase the
length extrapolation capability of Transformers,
previous work (Yao et al., 2021; Su, 2021) scales
the temperature of Softmax logarithmically w.r.t
the sequence length to ensure invariant entropy.
Our entropy alignment strategy is also inspired by
this line of research except that we adopt a differ-
ent procedure outlined later in Algorithm 1. Inter-
estingly, our experiment results in §5.2 show that
aligning by entropy performs worse than the other
proposed maximum probability alignment strategy.

3 Long-context Retrieval Tasks with T5

3.1 Why Retrieval?

As suggested by recent work (Mohtashami and
Jaggi, 2023; Li et al., 2023), the task of long-
context retrieval serves as a controllable bench-
mark to measure how well a Transformer language
model utilizes long-context inputs. One prominent
characteristic of retrieval tasks is that only a subset
of the input is of interest, requiring a model to accu-

(a) 1st Attention Head

(b) 27nd Attention Head

Figure 1: Visualization of T5 Positional Embeddings. To plot figures of bm,n, we set m = 7500 and vary the
value of n from 0 to 15k. Each attention head of a Flan-T5-XL encoder learns a set of positional embeddings that
capture different attention bias. For example, the positional embeddings in the left figure encourage the model to
focus on nearby tokens. In contrast, the ones in the right figure let the model focus on only remote tokens.

rately pick up the necessary information. The other
characteristic is that the key information can sit
anywhere in an input, requiring a model to attend
flexibly. Finally, the controllable aspect allows us
to gradually increase the input sequence length to
test models’ length extrapolation capability.

3.2 Why T5?

To solve retrieval tasks using Transformer language
models, it is necessary to choose a positional em-
bedding design that permits accurate and flexible
length-extrapolatable attention. After checking
through the existing positional embeddings in Ta-
ble 2, we find the T5 family (Raffel et al., 2020) fits
our needs. As for other candidates, learnable abso-
lute positional embeddings (Vaswani et al., 2017;
Zhang et al., 2022) must be evaluated within the
training length. ALiBi (Press et al., 2022) and Ro-
tary (Su et al., 2021) have a recency bias, and they
cannot extrapolate easily without fine-tuning.

For each attention head, T5 encoder maintains a
bucket (B) of 32 learnable parameters and assigns
the relative positional bias (rpe bias) bm,n as2

bm,n =

B[m − n], if 0 ≤ m − n < 8

B[n − m + 16], if − 8 < m − n < 0
B[min(15, 8 + ⌊ log((m−n)/8)

B[min(31, 24 + ⌊ log((n−m)/8)

log(128/8)

log(128/8)

· 8⌋)], if 8 ≤ m − n

· 8⌋)], if m − n ≤ −8,

where 0 ≤ m < L and 0 ≤ n < L are two
position indices. bm,n will be added to the (m, n)-

2https://github.com/huggingface/transformers/

blob/v4.33.2/src/transformers/models/t5/
modeling_t5.py#L390

th entry of the L × L self-attention matrix. The
summation becomes the input to the temperature-
scaled Softmax. We plot the learned rpe bias of a
T5 encoder in Figure 1. We can tell that its attention
heads encode rich attention patterns. For example,
head 1 learns to focus on the nearby tokens whereas
head 27 learns to ignore the nearby tokens and
allow access to faraway tokens.

3.3 The Dispersed Attention Issue of T5

Encoder

Unfortunately, directly applying T5 models on re-
trieval tasks does not yield perfect results. Upon
inspecting the intermediate model states, we find
that a longer input sequence consists of more to-
kens competing for the same amount (i.e., Softmax
sums to 1) of attention, resulting in the dispersed at-
tention issue. In Table 1, we see that the longer the
input sequence, the flatter the self-attention distri-
bution. The situation is not hopeless if the desired
information still attains a higher attention weight
than the remaining tokens. Our proposed solution
in §4 will let the key information stand out.

4 Proposed Methods

A natural solution to the dispersed attention issue
described in §3 is to sharpen the self-attention dis-
tribution, which can be achieved by reducing the
temperature τ during extrapolation. We set the ex-
trapolation temperature τex such that the sharpness
during training with τtr = 1 and that during extrap-
olation with τex < 1 are roughly the same. As a
measurement of sharpness, we explore the maxi-

Algorithm 1 Attention Alignment Strategies
Require: A short sequence of length Ltr and a
long sequence of length Lex. Encoder E. Align-
ment mode M .

Ensure: The Softmax temperature τ

function FINDS(τ , M )

to 0.5. We outline the procedure of the alignment
strategies in Algorithm 1. Note that our proposed
methods do not require any model fine-tuning or
gradient updates. Since the T5 model family was
pre-trained using sequences of length 512, we set
Ltr = 512 for all the experiments in this paper.

Set temperature of all Softmax to τ
s ← [ ]
for operation in E do

Perform the operation
if operation is Softmaxτ (l) then

if M is Maximum Probability then
Append max(Softmaxτ (l)) to s

else if M is Entropy then

Append H(Softmaxτ (l)) to s

end if

end if

end for
return avg(s)

end function
Str(1) ← FINDS(1.0, M )
for τex = 1.0, 0.95, 0.9, · · · , 0.5 do
Sex(τex) = FINDS(τex, M )

end for
return τex s.t. Sex(τex) ≈ Str(1)

mum probability or entropy of a distribution. In
other words, our proposed solution is to align the
maximum probability or entropy of training and
extrapolation distributions by adjusting τex.

Concretely, let l(i) ∈ RL be the i-th pre-Softmax
logit vector of a T5 encoder, where L ∈ {Ltr, Lex}
is the sequence length. The post-Softmax distri-
bution of l(i) is P(i)(τ ) = Softmaxτ (l(i)). The
maximum probability and entropy of P(i)(τ ) are
P(i)

max(τ ) and H(i)(τ ), respectively.
Let us take the maximum probability alignment
strategy as an example. We first run the for-
ward pass and compute the averaged maximum
probability under temperature τ over all logit vec-
tors: Pmax(τ ) = (1/N ) (cid:80)
max(τ ) where N =
R × H × L is the number of logit vectors in a
T5 encoder with R layers, H heads, and length-
L sequences. Since the temperature is 1 during
training and τex during extrapolation, we denote
the averaged maximum probability during train-
ing as Ptr
max(1) and that during extrapolation as
Pex
max(τex). Finally, to align the maximum probabil-
ity, we adjust τex such that Pex
max(1).
In practice, we do a grid search on τex from 1.0

max(τex) ≈ Ptr

i P(i)

5 Experiments

5.1 Language Modeling

Following previous work on Transformer length
extrapolation, we perform an intrinsic evaluation
on language modeling (Press et al., 2022; Chi et al.,
2022, 2023). Ideally, our proposed methods should
alleviate the perplexity explosion problem that hap-
pens during extrapolation. As we can see in Ta-
ble 3, both alignment strategies dramatically im-
prove (lower) the perplexity. We want to empha-
size that perplexity is not our primary focus and we
make no attempt to surpass previous work on this
metric since lower perplexity often cannot accu-
rately reflect the long-context utilization capability
of Transformers on practical tasks (Li et al., 2023).

Models

Sequence Length
1024 2048 4096 8192 15000
>1k
52.7
56.0
>1k
50.0
63.4
>1k
44.3
43.8

T5-Large-LM 35.9 40.1 >1k >1k
w/ Pmax
34.7 45.5 45.2 45.5
w/ H
40.2 43.9 45.6 54.6
T5-XL-LM 28.3 >1k >1k >1k
w/ Pmax
30.2 36.0 31.6 41.7
w/ H
30.4 36.8 38.4 53.3
T5-XXL-LM 109 >1k >1k >1k
w/ Pmax
32.2 29.7 29.5 36.6
w/ H
26.8 28.1 34.2 37.8

Table 3: Language Modeling Performance. Numbers
are perplexity. The lower the better. We use the LM-
Adapted T5 models for this experiment.3

5.2 Long-context Retrieval

Inspired by recently proposed retrieval tasks, we
evaluate the proposed alignment strategies on three
retrieval tasks. Topic retrieval requires a model to
retrieve the first topic in a long and multi-topic con-
versation (Li et al., 2023). Line retrieval has a long
series of key-value pairs, and a model needs to re-
trieve the value corresponding to the given key (Li
et al., 2023). Passkey retrieval hides a passkey in a

3https://github.com/google-research/

text-to-text-transfer-transformer/blob/main/
released_checkpoints.md#lm-adapted-t511lm100k

Retrieval Tasks

5

10

90
32

Models

20
97
94
0

Line, # of lines

Topic, # of topics
15
97 100 92
Flan-T5-Large 99 100 97
w/ Pmax
98
98
99
96
86
w/ H
94
90
59
97
16
77
90
100 100 100 100 100 96
Flan-T5-XL
w/ Pmax
89
90
100 100 100 100 100 97
w/ H
96
88
87
95
96
99
99 100 100 98
Flan-T5-XXL 100 100 100 99
w/ Pmax
96
99
97
99
100 100 100 99
w/ H
92
92
99
94
98
100 100 97
93
89 100 91
100 100 100 99
LongChat

Passkey, # of sentences
25 200 300 400 500 600 680 20k 30k 40k 50k 55k
9
83
93
62
92
85
98
98 100 84
88
93
22
3
29
26 100 100 100 100 100
45
62 100 99 100 100 100
70
71 100 100 100 100 100
70
82 100 100 100 100 100
84
95 100 98 100 100 100
94
58 100 100 100 100 100
58
59 100 100 99 100 99
78

96
98
83
57
80
79
95
97
76
83

31
85
21

47
90
25

16
79
15

98

97

Avg.

76
92
48
87
93
92
97
98
92
93

Table 4: Performance of Retrieval Tasks. The numbers are accuracy. Full score is 100. The LongChat model
corresponds to the LongChat-13B-16K model (Li et al., 2023). It is a LLaMA-13B (Touvron et al., 2023) model
fine-tuned on sequences of length 16k using the positional interpolation technique (Chen et al., 2023). Flan-T5-XXL
has 11B parameters. The maximum lengths of the three tasks are all around 14.5k to 15k tokens.

Topic, # of topics
15

Flan-T5-XL

w/ Pmax
w/ H

20

10

5
25 200 300 400 500 600 680 20k 30k 40k 50k 55k
0.8 0.75 0.7 0.65 0.65 0.8 0.75 0.75 0.7 0.70 0.7 0.85 0.8 0.75 0.75 0.75
0.7 0.55 0.55 0.5 0.5 0.6 0.55 0.55 0.5 0.5 0.5 0.7 0.65 0.6 0.6 0.6

Retrieval Tasks

Line, # of lines

Passkey, # of sentences

Table 5: Temperatures of Retrieval Tasks. We search the optimal temperature from 1.0, 0.95, 0.9, · · · , 0.5.

long junk text snippet, and a model needs to return
that passkey (Mohtashami and Jaggi, 2023). Since
the three tasks are formulated in the Question An-
swering (QA) format, we use the Flan-T5 models
to leverage their instruction-following capability.

As we can see in Table 4, the retrieval perfor-
mance is greatly boosted after the Flan-T5 models
are equipped with our proposed attention alignment
strategies. In particular, the maximum probability
alignment strategy gives us better results across the
board. Note that LongChat (Li et al., 2023), the
best baseline, was fine-tuned from LLaMA (Tou-
vron et al., 2023) on long sequences of length 16k
while our proposed methods do not need any fine-
tuning. Other baselines such as MPT (Team, 2023)
and ChatGLM2 (Du et al., 2022) perform worse
than LongChat. Please refer to Li et al. (2023) for
more details.

We also present the optimal temperature given
by Algorithm 1 in Table 5. As we can see, the
temperature indeed decreases when the input se-
quence length increases. The temperatures of the
other two model sizes are highly similar to the ones
presented in Table 5. Please find them in Table 8

in Appendix A. We will conduct more temperature
analysis later in §6.

5.3 Multi-document Question Answering

We choose the multi-document question answer-
ing task as our downstream task (Liu et al., 2023).
The model input consists of a questions and
multiple documents extracted from NaturalQues-
tions (Kwiatkowski et al., 2019) where one of the
documents (golden doc) contains the ground truth
answer. As we can see in Table 6, when a model is
equipped with the proposed maximum probability
alignment strategy, it consistently outperforms the
original model across model sizes and number of
input documents.

Apart from the better task performance, we be-
lieve the attention dispersed attention issue dis-
cussed in §3 can help demystify the lost-in-the-
middle phenomenon (Liu et al., 2023) of this task:
Transformer models tend to perform worse when
the ground truth sits near the middle of the input
context. Let us recall the relative positional embed-
ding of head 27 learned in Figure 1, if the ground
truth answer sits in the middle, it will have long

Models

Flan-T5-Large
w/ Pmax
Improvement
w/ H
Flan-T5-XL
w/ Pmax
Improvement
w/ H
Flan-T5-XXL
w/ Pmax
Improvement
w/ H

10 Docs
Avg.
52.4
53.1
+0.7
52.1
59.4
61.1
+1.7
60.5
63.6
63.7
+0.1
63.6

19
33.9
35.9

9
36.5
39.5

30 Docs, golden doc at different positions

Multi-document Question Answering
20 Docs
Avg.
43.3
44.2
+0.9
43.2
51.2
53.6
+2.4
52.4
56.9
57.7
+0.8
57.1

29
4
14
24
0
37.9
42.0
34.0
33.9
52.6
37.0
35.8
36.4
44.5
50.8
-0.9
+1.5 +3.0 +2.4 +2.0 +1.9
-1.8
34.2
33.3
33.5
41.1
47.6
54.8
46.4
39.9
44.6
58.4
60.9
55.7
49.1
44.9
49.1
+2.5 +4.5 +6.0 +5.0 +4.6 +2.7 +0.9
42.9
51.3
40.3
52.4
53.1
61.2
47.5
58.9
59.1
60.4
53.5
50.2
-2.1
+1.5 +3.4 +2.9 +2.7 +2.4 +0.4
55.7
51.9
61.0

35.2
40.0
46.0

42.1
48.1
51.0

32.2
41.7
46.3

43.5
49.1
52.5

42.0
48.9
51.3

50.7

50.3

53.4

50.8

Avg.
38.7
40.0
+1.3
36.7
46.5
50.3
+3.8
44.9
52.4
54.0
+1.6
53.4

Table 6: Performance of Multi-document QA. The numbers are accuracy. Full score is 100. The improvement
row represents the absolute accuracy improvement after a Flan-T5 model is equipped with our proposed maximum
probability alignment strategy. For the full performance breakdown, please refer to Table 9 in Appendix A.

contexts from both sides competing for the atten-
tion weight. If this hypothesis holds true, we expect
the performance boost to be prominent for the cases
where the answer sits near the middle. We reveal
the performance breakdown when the number of
input documents is 30. As we can see in the im-
provement row, those cases indeed achieve greater
improvement.

Our strategies are not always perfect: The per-
formance drops if the ground truth answer is at
position 29. We believe T5 might have already han-
dled this case pretty well due to the recency bias
learned on some attention heads, and our additional
temperature scaling sharpens the distribution too
aggressively. We acknowledge this trade-off in §8.

5.4 Overall Observation

The maximum probability alignment strategy is the
most reliable and best-performing method across
all tasks and settings, echoing our discussion in
§3.1: For most data, only a subset of the input is
useful. The maximum probability alignment strat-
egy captures this characteristic naturally, thereby
outperforming the entropy alignment strategy that
cares more about the holistic distribution.

6 Theoretical Analysis

To shed more light on the underlying mechanisms
of the two alignment strategies, we establish the
connection between the softmax temperature τ and
data distribution under empirically verified assump-

tions. We focus on the 0-th layer (closest to the
input embeddings) and take the average over all
logit vectors across attention heads. Note that this
is just a crude approximation of Algorithm 1 for
analysis purposes since 1) a Transformer language
model typically encompasses multiple layers, and
2) in Algorithm 1, we take the maximum probabil-
ity or entropy of individual logit vectors as opposed
to the averaged one.

To compute the averaged logit vector, we start
with a input sequence of length L. Using a Trans-
former model with H attention heads (specifically,
a T5 Encoder in our context), we generate H × L
pre-softmax logit vectors, each with a length of L.
Here, the number of layers is 1 because we focus
on the 0-th layer. These logit vectors are then indi-
vidually sorted, and we subsequently calculate the
average of all H × L sorted logit vectors, resulting
in the averaged logit vector of length L.

Assumption 1. The length L averaged logit vector
is normally distributed, i.e., its entry li ∼ N (0, σ2).

To assess whether the averaged logit entries fol-
low a Gaussian distribution, we employ QQ plots,
as illustrated in Figure 2. A QQ plot (Wilk and
Gnanadesikan, 1968) is a graphical technique used
for comparing two probability distributions by plot-
ting their quantiles against each other. A point (x,
y) corresponds to a quantile from the second dis-
tribution (y-coordinate) plotted against the same
quantile from the first distribution (x-coordinate).

(a) Short sequences around 512

(b) Long sequences around 15k

Figure 2: QQ plots of Flan-T5-XL. We experiment with short and long sequences. The red reference line is y=x.

Retrieval Tasks
Line

Topic

Passkey

512
8.61

15k
8.80

512
8.71

15k
8.97

512
8.75

15k
8.85

Criteria

lmax

Table 7: Largest Logit Entry of Flan-T5-XL. lmax is
the largest logit entry of the averaged logit vector. It is
consistent across different input sequence lengths.

When the two distributions under comparison are
similar, the points in the QQ plot will roughly align
with the identity line, y = x. In our case, where
we aim to determine the degree of Gaussian be-
havior in the averaged logit vector, the linearity
of the plot serves as an indicator – the closer the
points are to the identity line, the more Gaussian
the distribution.
Assumption 2. The largest logit entry of the aver-
aged logit vector during training and extrapolation
max = ltr
are the same: lex
max.
This is verified in Table 7.

6.1 Maximum Probability Alignment

Let lmax be the largest value in the logit vector l.
Let τ be the temperature of the Softmax function.
The probability of the largest entry is

Pmax =

elmax/τ
i=1 eli/τ

(cid:80)L

.

Since Softmax is shift-invariant, the logit vector
can always be made zero-mean: (cid:80)
i li = 0. Next,
according to Assumption 1, the denominator of
Softmax can be approximated as

L
(cid:88)

i=1

eli/τ ≈ L · E[eli/τ ] = L · eσ2/(2τ 2)

(1)

This implies Pmax is approximately

Pmax ≈

elmax/τ
Leσ2/(2τ 2)

During the training stage, the temperature τ is 1

Ptr

max ≈

max

eltr
Ltreσ2

tr/2

,

which gives an expression of the largest logit entry
during the training stage

ltr
max ≈ log

(cid:16)

maxLtreσ2
Ptr

tr/2(cid:17)

(2)

According to Assumption 2, the largest probability
during the extrapolation stage can be simplified as

A. 2=

max/τ

eltr
Lexeσ2

ex/(2τ 2)

Pex

max ≈

(cid:16)

(2)
≈

max/τ

elex
Lexeσ2
maxLtreσ2
Ptr
Lexeσ2

ex/(2τ 2)

ex/(2τ 2)
tr/2(cid:17)1/τ

Since τ is a free parameter during extrapolation,
we adjust it to carry out the maximum probability
alignment strategy.

Proposition 1. Under Assumption 1 and 2, we
can adjust the temperature τ to align the maximum
probability Ptr
max = P

max = Pex
log Ltr + log P + σ2

log Lex + log P + σ2

tr/2
ex/(2τ 2)

.

τ ≈

=

B
A + C
τ 2

=

Bτ 2
Aτ 2 + C

.

Assuming τ ̸= 0, we can solve the quadratic equa-
tion Aτ 2 − Bτ + C = 0 to get τ . We always pick
the larger root as our final solution.

6.2 Entropy Alignment

The entropy of a discrete probability computed by
Softmax is

H = −

eli/τ
D

(cid:88)

i

log

eli/τ
D

= log D −

(cid:80)

i

li
τ eli/τ
D

,

where D = (cid:80)
i eli/τ is the denominator of Soft-
max, which can be approximated using Eq. (1). On
the other hand, we note that (cid:80)
i lieli ≈ LE[lel].
When l ∼ N (0, σ2), E[lel] is approximated as (see
detailed derivation in Appendix A)

E[lel] =

(cid:90) ∞

−∞

σ

lel
√

2π

e− l2

2σ2 dl = eσ2/2σ2

(3)

Thus, combining Eq. (1) and (3), the entropy H is
approximated as

H ≈ log L +

= log L −

σ2
2τ 2 −
σ2
2τ 2

Leσ2/(2τ 2) σ2
τ 2
Leσ2/(2τ 2)

Since τ is set to 1 during the training stage, we
have Htr ≈ log Ltr − σ2
2 . During extrapolation,
tr
we align the entropy (i.e., Hex = Htr) by adjusting
τ .

log Lex −

σ2
ex
2τ 2 ≈ Hex = Htr ≈ log Ltr −

σ2
tr
2

.

Since τ is a free parameter during extrapolation, we
adjust it to carry out the entropy alignment strategy.

Proposition 2. Under Assumption 1, we can adjust
the temperature τ to align the entropy Htr = Hex

σex

τ ≈

(cid:113)

tr + 2 log Lex
σ2
Ltr

6.3 Real-world Temperatures

We verify Proposition 1 and 2 on the topic retrieval
task by plotting the temperature curves in Figure 3.
We empirically evaluate σtr at the training length
and σex every extrapolation length considering only
the 0-th layer. Please find the temperature curves
for the other two retrieval tasks in Appendix A.

While both proposed alignment strategies lower
the temperature when the input sequence length
increases, the entropy alignment strategy does so
more aggressively. We believe this leads to its in-
ferior performance observed in Table 4 and 6 (w/

Figure 3: Topic Retrieval Temperature Analysis.
Curves are given by Proposition 1 and 2. Cross signs
and dots are given by Algorithm 1. logL 512 is given
by Yao et al. (2021); Su (2021).

H). In addition, the real temperatures given by Al-
gorithm 1 is always higher than those given by the
two propositions. After checking the per-layer at-
tention distributions, we find that the 0-th layer has
flatter distributions compared to higher layers. Be-
cause the two propositions are derived based on the
0-th layer and a flatter distribution needs a lower
temperature to correct, the temperatures given by
them tend to be lower than the ones given by Al-
gorithm 1 that takes the average of temperatures
across all layers.

Finally, the previously proposed log-decaying
rule (τ = logLex Ltr) (Yao et al., 2021; Su, 2021)
was motivated by the idea of entropy invariance,
but our analysis shows that it is more similar to the
maximum probability alignment strategy (Proposi-
tion 1).

7 Conclusion

In this paper, we find that the T5 model family has
great potential when it comes to Transformer length
extrapolation. We propose the maximum probabil-
ity and entropy alignment strategies to fix T5’s
dispersed attention issue without model fine-tuning.
We conduct experiments on natural language mod-
eling, retrieval tasks, and multi-document question
answering to demonstrate the effectiveness of our
proposed methods. Finally, we present a simpli-
fied theoretical analysis to elucidate how the tem-
perature is scaled to achieve attention alignment.
We hope that our work can inspire future length-
extrapolatable Transformer designs.

8 Limitations

We base our theoretical analysis on a simplified
Transformer language model, which might be fur-
ther improved by taking all the layers and their
interactions into account. In addition, we find that
different layers have different degrees of distribu-
tion flatness, which could be leveraged in future
work to perform per-layer attention alignment. Fi-
nally, our temperature scaling scheme sometimes
sharpen a distribution too aggressively as discussed
in the multi-document question answering experi-
ments. This drawback could be possibly improved
by designing a more fine-grained attention align-
ment strategy.

Acknowledgment

The first author acknowledges the support from the
Boeing Company (2019-STU-PA-259).

References

Salesforce AI. 2023. Long sequence modeling with
xgen: A 7b llm trained on 8k input sequence length.

Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. 2023.
Pythia: A suite for analyzing large language mod-
In International
els across training and scaling.
Conference on Machine Learning, pages 2397–2430.
PMLR.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning, pages 2206–2240. PMLR.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
arXiv preprint arXiv:2306.15595.

Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and
Alexander I Rudnicky. 2022. KERPLE: Kernelized
Relative Positional Embedding for Length Extrapola-
tion. In Advances in Neural Information Processing
Systems (NeurIPS), New Orleans, USA.

Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky,
and Peter J Ramadge. 2023. Dissecting Transformer
Length Extrapolation via the Lens of Receptive Field
Analysis. In Annual Meeting of the Association for
Computational Linguistics (ACL), Toronto, Canada.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems,
35:16344–16359.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320–335.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning, pages 3929–3938. PMLR.

Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open do-
main question answering. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume,
pages 874–880, Online. Association for Computa-
tional Linguistics.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics, 7:453–
466.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems, 33:9459–9474.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph E Gonzalez, Ion Stoica, Xuezhe
Ma, and Hao Zhang. 2023. How long can opensource
llms truly promise on context length.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023.
in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172.

Lost

Amirkeivan Mohtashami and Martin Jaggi. 2023.
Landmark attention: Random-access infinite con-
arXiv preprint
text
transformers.
arXiv:2305.16300.

length for

Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The refinedweb dataset
for falcon llm: outperforming curated corpora with
arXiv preprint
web data, and web data only.
arXiv:2306.01116.

An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068.

Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations.

Markus N Rabe and Charles Staats. 2021. Self-attention
arXiv preprint

does not need o (n2) memory.
arXiv:2112.05682.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485–5551.

Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.

Jianlin Su. 2021. Scaling attention via the lens of en-

tropy invariance.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2021. Roformer: En-
hanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864.

The MosaicML NLP Team. 2023.

Introducing mpt-
7b: A new standard for open-source, commercially
usable llms. https://www.mosaicml.com/blog/mpt-
30b.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Martin B Wilk and Ram Gnanadesikan. 1968. Probabil-
ity plotting methods for the analysis for the analysis
of data. Biometrika, 55(1):1–17.

Shunyu Yao, Binghui Peng, Christos Papadimitriou,
and Karthik Narasimhan. 2021. Self-attention net-
works can process bounded hierarchical languages.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
3770–3785, Online. Association for Computational
Linguistics.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:

Figure 4: Line Retrieval Temperature Analysis.
Curves are given by Proposition 1 and 2. Cross signs
and dots are given by Algorithm 1. logL 512 is given
by Yao et al. (2021); Su (2021).

A Appendix

A.1 More Real-world Temperatures

We verify Proposition 1 and 2 on the remaining two
retrieval task by plotting the temperature curves in
Figure 4 and 5. We empirically evaluate σtr at the
training length and σex every extrapolation length
considering only the 0-th layer.

A.2 Detailed Expansion of Eq. (3)
(cid:90) ∞

lel
√

e− l2

2σ2 dl

E[lel] =

σ

2π
2σ2l−l2

e

2σ2 dl

−∞
l
√

2π

(cid:90) ∞

σ

−∞
(cid:90) ∞

=

=

−∞
= eσ2/2

2π

σ
(cid:90) ∞

−∞

σ

e− (l−σ2)2

2σ2 dl

l
√

2π

l
√

e− (l−σ2)2−σ4

2σ2

dl

(4)

= eσ2/2σ2

A.3 More Temperatures of Retrieval Tasks

We report the temperatures of the three retrieval
tasks across model sizes given by Algorithm 1 in
Table 8.

A.4 Breakdown of Multi-document Question

Answering

We report the performance breakdown of different
number of input documents in Table 9.

Figure 5: Passkey Retrieval Temperature Analysis.
Curves are given by Proposition 1 and 2. Cross signs
and dots are given by Algorithm 1. logL 512 is given
by Yao et al. (2021); Su (2021).

Models

Flan-T5-Large
w/ Pmax
w/ H
Flan-T5-XL
w/ Pmax
w/ H
Flan-T5-XXL
w/ Pmax
w/ H

Topic, # of topics
15

20

10

5

Retrieval Tasks

Line, # of lines

Passkey, # of sentences

25 200 300 400 500 600 680 20k 30k 40k 50k 55k

0.85 0.8 0.75 0.75 0.75 0.85 0.8 0.8 0.75 0.75 0.75 0.85 0.80 0.80 0.75 0.75
0.7 0.6 0.55 0.5 0.5 0.65 0.55 0.55 0.5 0.5 0.5 0.6 0.55 0.5 0.5 0.5

0.8 0.75 0.7 0.65 0.65 0.8 0.75 0.75 0.7 0.70 0.7 0.85 0.8 0.75 0.75 0.75
0.7 0.55 0.55 0.5 0.5 0.6 0.55 0.55 0.5 0.5 0.5 0.7 0.65 0.6 0.6 0.6

0.85 0.8 0.75 0.75 0.75 0.8 0.8 0.75 0.75 0.75 0.75 0.85 0.8 0.8 0.75 0.75
0.75 0.65 0.6 0.55 0.55 0.65 0.6 0.6 0.55 0.55 0.55 0.65 0.6 0.55 0.55 0.5

Table 8: Temperatures of Retrieval Tasks. We search the optimal temperature from 1.0, 0.95, 0.9, · · · , 0.5.

Multi-document Question Answering

30 Docs
14

Models

10 Docs
4

20 Docs
9

0

4

9

4

0

0

9

19

14

29
Flan-T5-Large 60.6 48.5 48.0 54.5 44.0 39.6 38.0 40.2 52.6 42.0 36.5 34.0 33.9 33.9 37.9
60.9 49.8 48.6 53.5 45.6 40.8 39.7 41.3 50.8 44.5 39.5 36.4 35.9 35.8 37.0
w/ Max.
58.9 50.1 47.3 52.4 45.2 40.4 38.00 40.0 47.6 41.1 35.2 33.5 32.2 33.3 34.2
w/ Ent.
64.0 55.4 58.9 60.6 47.9 45.1 47.3 55.3 58.4 44.6 40.0 39.9 41.7 46.4 54.8
Flan-T5-XL
65.3 57.3 60.8 62.2 51.6 49.0 49.4 56.0 60.9 49.1 46.0 44.9 46.3 49.1 55.7
w/ Max.
w/ Ent.
64.7 56.7 60.0 59.3 50.1 47.9 49.8 55.1 52.4 43.5 42.1 40.3 42.0 42.9 51.3
Flan-T5-XXL 65.1 61.0 64.6 61.1 53.9 52.4 54.7 62.4 58.9 49.1 48.1 47.5 48.9 53.1 61.2
66.2 61.8 63.2 62.8 55.9 54.4 55.6 59.6 60.4 52.5 51.0 50.2 51.3 53.5 59.1
w/ Max.
67.3 62.1 61.3 63.2 56.1 54.1 54.3 57.6 61.0 53.4 50.8 50.3 50.7 51.9 55.7
w/ Ent.

19

24

Table 9: Full Performance Breakdown of Retrieval Tasks. The numbers are accuracy. Full score is 100. 0, 4, 9...
indicate the position of the golden document that contains the answer to a question.

