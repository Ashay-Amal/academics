Title,ID,URL,Summary
Humanoid Locomotion as Next Token Prediction,http://arxiv.org/abs/2402.19469v1,http://arxiv.org/pdf/2402.19469v1,"We cast real-world humanoid control as a next token prediction problem, akin
to predicting the next word in language. Our model is a causal transformer
trained via autoregressive prediction of sensorimotor trajectories. To account
for the multi-modal nature of the data, we perform prediction in a
modality-aligned way, and for each input token predict the next token from the
same modality. This general formulation enables us to leverage data with
missing modalities, like video trajectories without actions. We train our model
on a collection of simulated trajectories coming from prior neural network
policies, model-based controllers, motion capture data, and YouTube videos of
humans. We show that our model enables a full-sized humanoid to walk in San
Francisco zero-shot. Our model can transfer to the real world even when trained
on only 27 hours of walking data, and can generalize to commands not seen
during training like walking backward. These findings suggest a promising path
toward learning challenging real-world control tasks by generative modeling of
sensorimotor trajectories."
Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm,http://arxiv.org/abs/2402.19456v1,http://arxiv.org/pdf/2402.19456v1,"The quantum approximate optimization algorithm (QAOA) is a general-purpose
algorithm for combinatorial optimization. In this paper, we analyze the
performance of the QAOA on a statistical estimation problem, namely, the spiked
tensor model, which exhibits a statistical-computational gap classically. We
prove that the weak recovery threshold of $1$-step QAOA matches that of
$1$-step tensor power iteration. Additional heuristic calculations suggest that
the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor
power iteration when $p$ is a fixed constant. This further implies that
multi-step QAOA with tensor unfolding could achieve, but not surpass, the
classical computation threshold $\Theta(n^{(q-2)/4})$ for spiked $q$-tensors.
  Meanwhile, we characterize the asymptotic overlap distribution for $p$-step
QAOA, finding an intriguing sine-Gaussian law verified through simulations. For
some $p$ and $q$, the QAOA attains an overlap that is larger by a constant
factor than the tensor power iteration overlap. Of independent interest, our
proof techniques employ the Fourier transform to handle difficult combinatorial
sums, a novel approach differing from prior QAOA analyses on spin-glass models
without planted structure."
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models,http://arxiv.org/abs/2402.19449v1,http://arxiv.org/pdf/2402.19449v1,"Adam has been shown to outperform gradient descent in optimizing large
language transformers empirically, and by a larger margin than on other tasks,
but it is unclear why this happens. We show that the heavy-tailed class
imbalance found in language modeling tasks leads to difficulties in the
optimization dynamics. When training with gradient descent, the loss associated
with infrequent words decreases slower than the loss associated with frequent
ones. As most samples come from relatively infrequent words, the average loss
decreases slowly with gradient descent. On the other hand, Adam and sign-based
methods do not suffer from this problem and improve predictions on all classes.
To establish that this behavior is indeed caused by class imbalance, we show
empirically that it persist through different architectures and data types, on
language transformers, vision CNNs, and linear models. We further study this
phenomenon on a linear classification with cross-entropy loss, showing that
heavy-tailed class imbalance leads to ill-conditioning, and that the
normalization used by Adam can counteract it."
