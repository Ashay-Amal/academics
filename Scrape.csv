Title,ID,URL,Summary
Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction,http://arxiv.org/abs/2404.19630v1,http://arxiv.org/pdf/2404.19630v1,"The rapid rise of deep learning (DL) in numerical weather prediction (NWP)
has led to a proliferation of models which forecast atmospheric variables with
comparable or superior skill than traditional physics-based NWP. However, among
these leading DL models, there is a wide variance in both the training settings
and architecture used. Further, the lack of thorough ablation studies makes it
hard to discern which components are most critical to success. In this work, we
show that it is possible to attain high forecast skill even with relatively
off-the-shelf architectures, simple training procedures, and moderate compute
budgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5
data, and find that it attains superior forecast skill when compared against
IFS. We present some ablations on key aspects of the training pipeline,
exploring different loss functions, model sizes and depths, and multi-step
fine-tuning to investigate their effect. We also examine the model performance
with metrics beyond the typical ACC and RMSE, and investigate how the
performance scales with model size."
A Recipe for CAC: Mosaic-based Generalized Loss for Improved Class-Agnostic Counting,http://arxiv.org/abs/2404.09826v1,http://arxiv.org/pdf/2404.09826v1,"Class agnostic counting (CAC) is a vision task that can be used to count the
total occurrence number of any given reference objects in the query image. The
task is usually formulated as a density map estimation problem through
similarity computation among a few image samples of the reference object and
the query image. In this paper, we point out a severe issue of the existing CAC
framework: Given a multi-class setting, models don't consider reference images
and instead blindly match all dominant objects in the query image. Moreover,
the current evaluation metrics and dataset cannot be used to faithfully assess
the model's generalization performance and robustness. To this end, we discover
that the combination of mosaic augmentation with generalized loss is essential
for addressing the aforementioned issue of CAC models to count objects of
majority (i.e. dominant objects) regardless of the references. Furthermore, we
introduce a new evaluation protocol and metrics for resolving the problem
behind the existing CAC evaluation scheme and better benchmarking CAC models in
a more fair manner. Besides, extensive evaluation results demonstrate that our
proposed recipe can consistently improve the performance of different CAC
models. The code will be released upon acceptance."
The Need for Speed: Pruning Transformers with One Recipe,http://arxiv.org/abs/2403.17921v1,http://arxiv.org/pdf/2403.17921v1,"We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$."
