Title,ID,URL,Summary
UWAFA-GAN: Ultra-Wide-Angle Fluorescein Angiography Transformation via Multi-scale Generation and Registration Enhancement,http://arxiv.org/abs/2405.00542v1,http://arxiv.org/pdf/2405.00542v1,"Fundus photography, in combination with the ultra-wide-angle fundus (UWF)
techniques, becomes an indispensable diagnostic tool in clinical settings by
offering a more comprehensive view of the retina. Nonetheless, UWF fluorescein
angiography (UWF-FA) necessitates the administration of a fluorescent dye via
injection into the patient's hand or elbow unlike UWF scanning laser
ophthalmoscopy (UWF-SLO). To mitigate potential adverse effects associated with
injections, researchers have proposed the development of cross-modality medical
image generation algorithms capable of converting UWF-SLO images into their
UWF-FA counterparts. Current image generation techniques applied to fundus
photography encounter difficulties in producing high-resolution retinal images,
particularly in capturing minute vascular lesions. To address these issues, we
introduce a novel conditional generative adversarial network (UWAFA-GAN) to
synthesize UWF-FA from UWF-SLO. This approach employs multi-scale generators
and an attention transmit module to efficiently extract both global structures
and local lesions. Additionally, to counteract the image blurriness issue that
arises from training with misaligned data, a registration module is integrated
within this framework. Our method performs non-trivially on inception scores
and details generation. Clinical user studies further indicate that the UWF-FA
images generated by UWAFA-GAN are clinically comparable to authentic images in
terms of diagnostic reliability. Empirical evaluations on our proprietary UWF
image datasets elucidate that UWAFA-GAN outperforms extant methodologies. The
code is accessible at https://github.com/Tinysqua/UWAFA-GAN."
Exploring Self-Supervised Vision Transformers for Deepfake Detection: A Comparative Analysis,http://arxiv.org/abs/2405.00355v1,http://arxiv.org/pdf/2405.00355v1,"This paper investigates the effectiveness of self-supervised pre-trained
transformers compared to supervised pre-trained transformers and conventional
neural networks (ConvNets) for detecting various types of deepfakes. We focus
on their potential for improved generalization, particularly when training data
is limited. Despite the notable success of large vision-language models
utilizing transformer architectures in various tasks, including zero-shot and
few-shot learning, the deepfake detection community has still shown some
reluctance to adopt pre-trained vision transformers (ViTs), especially large
ones, as feature extractors. One concern is their perceived excessive capacity,
which often demands extensive data, and the resulting suboptimal generalization
when training or fine-tuning data is small or less diverse. This contrasts
poorly with ConvNets, which have already established themselves as robust
feature extractors. Additionally, training and optimizing transformers from
scratch requires significant computational resources, making this accessible
primarily to large companies and hindering broader investigation within the
academic community. Recent advancements in using self-supervised learning (SSL)
in transformers, such as DINO and its derivatives, have showcased significant
adaptability across diverse vision tasks and possess explicit semantic
segmentation capabilities. By leveraging DINO for deepfake detection with
modest training data and implementing partial fine-tuning, we observe
comparable adaptability to the task and the natural explainability of the
detection result via the attention mechanism. Moreover, partial fine-tuning of
transformers for deepfake detection offers a more resource-efficient
alternative, requiring significantly fewer computational resources."
Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph,http://arxiv.org/abs/2405.00352v1,http://arxiv.org/pdf/2405.00352v1,"Temporal Knowledge Graph (TKG) reasoning often involves completing missing
factual elements along the timeline. Although existing methods can learn good
embeddings for each factual element in quadruples by integrating temporal
information, they often fail to infer the evolution of temporal facts. This is
mainly because of (1) insufficiently exploring the internal structure and
semantic relationships within individual quadruples and (2) inadequately
learning a unified representation of the contextual and temporal correlations
among different quadruples. To overcome these limitations, we propose a novel
Transformer-based reasoning model (dubbed ECEformer) for TKG to learn the
Evolutionary Chain of Events (ECE). Specifically, we unfold the neighborhood
subgraph of an entity node in chronological order, forming an evolutionary
chain of events as the input for our model. Subsequently, we utilize a
Transformer encoder to learn the embeddings of intra-quadruples for ECE. We
then craft a mixed-context reasoning module based on the multi-layer perceptron
(MLP) to learn the unified representations of inter-quadruples for ECE while
accomplishing temporal knowledge reasoning. In addition, to enhance the
timeliness of the events, we devise an additional time prediction task to
complete effective temporal information within the learned unified
representation. Extensive experiments on six benchmark datasets verify the
state-of-the-art performance and the effectiveness of our method."
