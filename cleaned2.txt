attention alignment flexible positional embeddings improve transformer length extrapolation tachung chi carnegie mellon university tachungcandrewcmuedu tinghan fan independent researcher tinghanfalumniprincetonedu alexander rudnicky carnegie mellon university aircscmuedu abstract ideal lengthextrapolatable transformer lan guage model handle sequences longer training length long sequence finetuning longcontext utilization pability highly relies flexible positional embedding design investigating flex ibility existing large pretrained transformer language models find family deserves closer look positional embed dings capture rich flexible attention pat terns suffers dispersed attention issue longer input sequence flatter attention distribution alleviate issue propose attention alignment strategies temperature scaling findings improve longcontext utilization capabil ity language modeling retrieval multidocument question answering finetuning suggesting flexible sitional embedding design attention align ment long way transformer length extrapolation1 introduction pretraining large transformer language models long sequences inherently expensive selfattentions quadratic complexity wrt sequence length vaswani al 2017 help memoryefficient attention rabe staats 2021 dao al 2022 maximum supported input length current opensource pre trained transformer language models capped 4096 tokens touvron al 2023 limiting efficacy handling longcontext tasks notable research topic aiming lift input length restriction length extrapo lation press al 2022 ideally length extrapolatable transformer language model trained short sequences perform equally 1httpsgithubcomchijames retrieval tasks line topic passkey 512 028 347 15k 012 663 512 027 347 15k 011 704 512 032 309 15k 024 597 criteria pmax table dispersed attention issue flant5 encoder pmax averaged maximum proba bility averaged entropy increasing sequence length 512 15k observe larger entropy smaller maximum probability implying flatter selfattention distribution longer ones finetuning possible carefully designed sitional embeddings press al 2022 chi al 2022 2023 unfortunately existing approaches tailored natural language modeling task known strong recency bias perform seemingly simple tasks passkey topic line retrieval htashami jaggi 2023 al 2023 circumvent recency bias sift positional embeddings existing opensource large pretrained transformer language models table find flexible design fam ily raffel al 2020 comes attention visualized figure flexibility t5s posi tional embeddings allows encourage recency bias head discourage head free lunch suffers dispersed attention issue shown ble nutshell attention distributions long input sequences tend flatter short input sequences remedy propose finetuningfree attention alignment strategies softmax temperature scaling yao al 2021 2021 mitigate dispersed attention sue maximum probability pmax entropy alignment attentionalignmenttransformerlengthextrapolation validate effectiveness alignment models 2020 opt 2022 chatglm 2022 llama 2023 falcon 2023 pythia 2023 xgen 2023 bloom 2022 mpt 2023 pe learned learned relative absolute rotary relative rotary relative rotary relative rotary relative rotary relative alibi relative alibi relative table opensource transformer language models positional embeddings model equipped learnable relative positional embeddings enable longcontext utilization capability strategies tasks including language model ing retrieval multidocument question answer ing provide theoretical analysis alignment strategies work hood investigating relation softmax tem perature data distribution related work transformer embeddings positional transformerbased models rely positional embeddings encode positional information summarize opensource large pretrained transformer language models positional embeddings table relative variants widely adopted better empirical performance al 2021 possible lengthextrapolatable capability press al 2022 work special focus positional embeddings flexibility shown figure transformer length extrapolation existing research transformer length extrapolation task natural language mod eling press al 2022 chi al 2022 2023 unfortunately reported positive results carry task longcontext retrieval htashami jaggi 2023 al 2023 contrastive observation explained mod els short empirical receptive field chi al 2023 short strong decaying prior positional beddings prevents models accessing faraway tokens necessary retrieval tasks work improve flexible positional embed dings limitation transformer position interpolation instead performing direct length extrapolation line research conducts model finetuning long input sequences chen al 2023 main focus identify efficient fine tuning scheme improves longcontext utiliza tion positive results reported retrieval tasks al 2023 argue finetuning incurs additional costs needs gpu resources perform long sequence fine tuning large models predefined target sequence length imposes sequence length upper limit proposed methods cir cumvent limitations tasks retrieval transformers transformerbased approaches consist retriever reader overcome context length restriction guu al 2020 lewis al 2020 izacard grave 2021 borgeaud al 2022 retriever retrieves relevant text snippets huge database reader digests retrieved information generate correct output proposed attention alignment strategy significantly increase input sequence length reader allowing retrieved information participate decision process smallscale retrieval problems methods obviates need context segmentation external keyvalue store prior work mohtashami jaggi 2023 serving elegant approach softmax temperature scaling increase length extrapolation capability transformers previous work yao al 2021 2021 scales temperature softmax logarithmically wrt sequence length ensure invariant entropy entropy alignment strategy inspired line research adopt differ ent procedure outlined later algorithm inter estingly experiment results 52 aligning entropy performs worse proposed maximum probability alignment strategy longcontext retrieval tasks 31 retrieval suggested recent work mohtashami jaggi 2023 al 2023 task long context retrieval serves controllable bench mark measure transformer language model utilizes longcontext inputs prominent characteristic retrieval tasks subset input interest requiring model accu 1st attention head 27nd attention head figure visualization positional embeddings plot figures bmn set 7500 vary value 15k attention head flant5xl encoder learns set positional embeddings capture different attention bias example positional embeddings left figure encourage model focus nearby tokens contrast ones right figure let model focus remote tokens rately pick necessary information characteristic key information sit input requiring model attend flexibly finally controllable aspect allows gradually increase input sequence length test models length extrapolation capability 32 solve retrieval tasks transformer language models necessary choose positional bedding design permits accurate flexible lengthextrapolatable attention checking existing positional embeddings ble find family raffel al 2020 fits needs candidates learnable abso lute positional embeddings vaswani al 2017 zhang al 2022 evaluated training length alibi press al 2022 tary al 2021 recency bias extrapolate easily finetuning attention head encoder maintains bucket learnable parameters assigns relative positional bias rpe bias bmn as2 bmn  bmin15 logmn8  bmin31 lognm8 log1288 log1288 position indices bmn added 2httpsgithubcomhuggingfacetransformers blobv4332srctransformersmodelst5 modelingt5pyl390 entry selfattention matrix summation input temperature scaled softmax plot learned rpe bias encoder figure tell attention heads encode rich attention patterns example head learns focus nearby tokens head learns ignore nearby tokens allow access faraway tokens 33 dispersed attention issue encoder unfortunately directly applying models trieval tasks yield perfect results inspecting intermediate model states find longer input sequence consists kens competing ie softmax sums attention resulting dispersed tention issue table longer input sequence flatter selfattention distri bution situation hopeless desired information attains higher attention weight remaining tokens proposed solution let key information stand out proposed methods natural solution dispersed attention issue described sharpen selfattention dis tribution achieved reducing temperature extrapolation set trapolation temperature ex sharpness training tr extrap olation ex roughly same measurement sharpness explore maxi algorithm attention alignment strategies require short sequence length ltr long sequence length lex encoder align ment mode ensure softmax temperature function finds 05 outline procedure alignment strategies algorithm note proposed methods require model finetuning gradient updates model family pretrained sequences length 512 set ltr 512 experiments paper set temperature softmax operation perform operation operation softmax maximum probability append maxsoftmax entropy append hsoftmax end end end return avgs end function str1 finds10 ex 10 095 09 05 sexex findsex end return ex st sexex str1 mum probability entropy distribution words proposed solution align maximum probability entropy training extrapolation distributions adjusting ex concretely let ith presoftmax logit vector encoder ltr lex sequence length postsoftmax distri bution pi softmax li maximum probability entropy pi max hi respectively let maximum probability alignment strategy example run ward pass compute averaged maximum probability temperature logit vec tors pmax cid80 max number logit vectors encoder layers heads length sequences temperature training ex extrapolation denote averaged maximum probability train ing ptr max1 extrapolation pex maxex finally align maximum probabil ity adjust ex pex max1 practice grid search ex 10 maxex ptr experiments 51 language modeling following previous work transformer length extrapolation perform intrinsic evaluation language modeling press al 2022 chi al 2022 2023 ideally proposed methods alleviate perplexity explosion problem hap pens extrapolation ble alignment strategies dramatically prove lower perplexity want empha size perplexity primary focus attempt surpass previous work metric lower perplexity accu rately reflect longcontext utilization capability transformers practical tasks al 2023 models sequence length 1024 2048 4096 8192 15000 527 560 500 634 443 438 t5largelm 359 401 pmax 347 455 452 455 402 439 456 546 t5xllm 283 pmax 302 360 316 417 304 368 384 533 t5xxllm 109 pmax 322 297 295 366 268 281 342 378 table language modeling performance numbers perplexity lower better use adapted models experiment3 52 longcontext retrieval inspired recently proposed retrieval tasks evaluate proposed alignment strategies retrieval tasks topic retrieval requires model retrieve topic long multitopic con versation al 2023 line retrieval long series keyvalue pairs model needs trieve value corresponding given key al 2023 passkey retrieval hides passkey 3httpsgithubcomgoogleresearch texttotexttransfertransformerblobmain releasedcheckpointsmdlmadaptedt511lm100k retrieval tasks models line lines topic topics 100 flant5large 100 pmax 100 100 100 100 100 flant5xl pmax 100 100 100 100 100 100 100 flant5xxl 100 100 100 pmax 100 100 100 100 100 100 100 100 100 longchat passkey sentences 200 300 400 500 600 680 20k 30k 40k 50k 55k 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 avg table performance retrieval tasks numbers accuracy score 100 longchat model corresponds longchat13b16k model al 2023 llama13b touvron al 2023 model finetuned sequences length 16k positional interpolation technique chen al 2023 flant5xxl 11b parameters maximum lengths tasks 145k 15k tokens topic topics flant5xl pmax 200 300 400 500 600 680 20k 30k 40k 50k 55k 08 075 07 065 065 08 075 075 07 070 07 085 08 075 075 075 07 055 055 05 05 06 055 055 05 05 05 07 065 06 06 06 retrieval tasks line lines passkey sentences table temperatures retrieval tasks search optimal temperature 10 095 09 05 long junk text snippet model needs return passkey mohtashami jaggi 2023 tasks formulated question swering format use flant5 models leverage instructionfollowing capability table retrieval perfor mance greatly boosted flant5 models equipped proposed attention alignment strategies particular maximum probability alignment strategy gives better results board note longchat al 2023 best baseline finetuned llama tou vron al 2023 long sequences length 16k proposed methods need fine tuning baselines mpt team 2023 chatglm2 al 2022 perform worse longchat refer al 2023 details present optimal temperature given algorithm table temperature decreases input quence length increases temperatures model sizes highly similar ones presented table find table appendix conduct temperature analysis later 6 53 multidocument question answering choose multidocument question answer ing task downstream task liu al 2023 model input consists questions multiple documents extracted naturalques tions kwiatkowski al 2019 documents golden doc contains ground truth answer table model equipped proposed maximum probability alignment strategy consistently outperforms original model model sizes number input documents apart better task performance lieve attention dispersed attention issue dis cussed help demystify lostinthe middle phenomenon liu al 2023 task transformer models tend perform worse ground truth sits near middle input context let recall relative positional embed ding head learned figure ground truth answer sits middle long models flant5large pmax improvement flant5xl pmax improvement flant5xxl pmax improvement docs avg 524 531 07 521 594 611 17 605 636 637 01 636 339 359 365 395 docs golden doc different positions multidocument question answering docs avg 433 442 09 432 512 536 24 524 569 577 08 571 379 420 340 339 526 370 358 364 445 508 09 15 30 24 20 19 18 342 333 335 411 476 548 464 399 446 584 609 557 491 449 491 25 45 60 50 46 27 09 429 513 403 524 531 612 475 589 591 604 535 502 21 15 34 29 27 24 04 557 519 610 352 400 460 421 481 510 322 417 463 435 491 525 420 489 513 507 503 534 508 avg 387 400 13 367 465 503 38 449 524 540 16 534 table performance multidocument qa numbers accuracy score 100 improvement row represents absolute accuracy improvement flant5 model equipped proposed maximum probability alignment strategy performance breakdown refer table appendix contexts sides competing atten tion weight hypothesis holds true expect performance boost prominent cases answer sits near middle reveal performance breakdown number input documents 30 provement row cases achieve greater improvement strategies perfect formance drops ground truth answer position 29 believe han dled case pretty recency bias learned attention heads additional temperature scaling sharpens distribution aggressively acknowledge tradeoff 8 54 overall observation maximum probability alignment strategy reliable bestperforming method tasks settings echoing discussion 31 data subset input useful maximum probability alignment strat egy captures characteristic naturally outperforming entropy alignment strategy cares holistic distribution theoretical analysis shed light underlying mechanisms alignment strategies establish connection softmax temperature data distribution empirically verified assump tions focus 0th layer closest input embeddings average logit vectors attention heads note crude approximation algorithm analysis purposes transformer language model typically encompasses multiple layers algorithm maximum probabil ity entropy individual logit vectors opposed averaged one compute averaged logit vector start input sequence length trans model attention heads specifically encoder context generate presoftmax logit vectors length number layers focus 0th layer logit vectors indi vidually sorted subsequently calculate average sorted logit vectors resulting averaged logit vector length assumption length averaged logit vector normally distributed ie entry 2 assess averaged logit entries fol low gaussian distribution employ plots illustrated figure plot wilk gnanadesikan 1968 graphical technique comparing probability distributions plot ting quantiles other point corresponds quantile second dis tribution ycoordinate plotted quantile distribution xcoordinate short sequences 512 long sequences 15k figure plots flant5xl experiment short long sequences red reference line yx retrieval tasks line topic passkey 512 861 15k 880 512 871 15k 897 512 875 15k 885 criteria lmax table largest logit entry flant5xl lmax largest logit entry averaged logit vector consistent different input sequence lengths distributions comparison similar points plot roughly align identity line case aim determine degree gaussian havior averaged logit vector linearity plot serves indicator closer points identity line gaussian distribution assumption largest logit entry aver aged logit vector training extrapolation max ltr lex max verified table 61 maximum probability alignment let lmax largest value logit vector let temperature softmax function probability largest entry pmax elmax eli cid80l softmax shiftinvariant logit vector zeromean cid80 according assumption denominator softmax approximated cid88 eli eeli e22 implies pmax approximately pmax elmax le22 training stage temperature ptr max max eltr ltre2 tr2 gives expression largest logit entry training stage ltr max log cid16 maxltre2 ptr tr2cid17 according assumption largest probability extrapolation stage simplified max eltr lexe2 ex2 pex max cid16 max elex lexe2 maxltre2 ptr lexe2 ex2 ex2 tr2cid171 free parameter extrapolation adjust carry maximum probability alignment strategy proposition assumption adjust temperature align maximum probability ptr max max pex log ltr log log lex log tr2 ex2 assuming solve quadratic equa tion pick larger root final solution 62 entropy alignment entropy discrete probability computed softmax eli cid88 log eli log cid80 eli cid80 eli denominator soft max approximated eq hand note cid80 lieli lelel elel approximated detailed derivation appendix elel cid90 lel 22 e222 combining eq entropy approximated log log le22 le22 set training stage htr log ltr extrapolation align entropy ie hex htr adjusting log lex hex htr log ltr free parameter extrapolation adjust carry entropy alignment strategy proposition assumption adjust temperature align entropy htr hex ex cid113 log lex ltr 63 realworld temperatures verify proposition topic retrieval task plotting temperature curves figure empirically evaluate tr training length ex extrapolation length considering 0th layer find temperature curves retrieval tasks appendix proposed alignment strategies lower temperature input sequence length increases entropy alignment strategy aggressively believe leads ferior performance observed table figure topic retrieval temperature analysis curves given proposition cross signs dots given algorithm logl 512 given yao al 2021 2021 addition real temperatures given gorithm higher given propositions checking perlayer tention distributions find 0th layer flatter distributions compared higher layers cause propositions derived based 0th layer flatter distribution needs lower temperature correct temperatures given tend lower ones given gorithm takes average temperatures layers finally previously proposed logdecaying rule loglex ltr yao al 2021 2021 motivated idea entropy invariance analysis shows similar maximum probability alignment strategy proposi tion conclusion paper find model family great potential comes transformer length extrapolation propose maximum probabil ity entropy alignment strategies fix t5s dispersed attention issue model finetuning conduct experiments natural language mod eling retrieval tasks multidocument question answering demonstrate effectiveness proposed methods finally present simpli fied theoretical analysis elucidate tem perature scaled achieve attention alignment hope work inspire future length extrapolatable transformer designs limitations base theoretical analysis simplified transformer language model fur ther improved taking layers interactions account addition find different layers different degrees distribu tion flatness leveraged future work perform perlayer attention alignment nally temperature scaling scheme sharpen distribution aggressively discussed multidocument question answering experi ments drawback possibly improved designing finegrained attention align ment strategy acknowledgment author acknowledges support boeing company 2019stupa259 references salesforce ai 2023 long sequence modeling xgen llm trained input sequence length stella biderman hailey schoelkopf quentin gregory anthony herbie bradley kyle obrien eric hal lahan mohammad aflah khan shivanshu purohit usvsn sai prashanth edward raff al 2023 pythia suite analyzing large language mod international els training scaling conference machine learning pages 23972430 pmlr sebastian borgeaud arthur mensch jordan hoff mann trevor cai eliza rutherford katie milli george van den driessche jeanbaptiste lespiau bogdan damoc aidan clark al 2022 improving language models retrieving tril lions tokens international conference chine learning pages 22062240 pmlr shouyuan chen sherman wong liangjian chen yuandong tian 2023 extending context window large language models positional interpolation arxiv preprint arxiv230615595 tachung chi tinghan fan peter ramadge alexander rudnicky 2022 kerple kernelized relative positional embedding length extrapola tion advances neural information processing systems neurips new orleans usa tachung chi tinghan fan alexander rudnicky peter ramadge 2023 dissecting transformer length extrapolation lens receptive field analysis annual meeting association computational linguistics acl toronto canada tri dao dan stefano ermon atri rudra christopher r 2022 flashattention fast memoryefficient exact attention ioawareness advances neural information processing systems 351634416359 zhengxiao yujie qian xiao liu ming ding jiezhong qiu zhilin yang jie tang 2022 glm general language model pretraining autoregres sive blank infilling proceedings 60th nual meeting association computational linguistics volume long papers pages 320335 kelvin guu kenton lee zora tung panupong pasu pat mingwei chang 2020 retrieval augmented language model pretraining international confer ence machine learning pages 39293938 pmlr gautier izacard edouard grave 2021 leveraging passage retrieval generative models open main question answering proceedings 16th conference european chapter associ ation computational linguistics main volume pages 874880 online association computa tional linguistics tom kwiatkowski jennimaria palomaki olivia red field michael collins ankur parikh chris alberti danielle epstein illia polosukhin jacob devlin ken ton lee al 2019 natural questions benchmark question answering research transactions association computational linguistics 7453 466 patrick lewis ethan perez aleksandra piktus fabio petroni vladimir karpukhin naman goyal hein rich kttler mike lewis wentau yih tim rock tschel al 2020 retrievalaugmented generation knowledgeintensive nlp tasks advances neu ral information processing systems 3394599474 dacheng rulin shao anze xie ying sheng lian min zheng joseph gonzalez ion stoica xuezhe hao zhang 2023 long opensource llms truly promise context length nelson liu kevin lin john hewitt ashwin paran jape michele bevilacqua fabio petroni percy liang 2023 middle lan guage models use long contexts arxiv preprint arxiv230703172 lost amirkeivan mohtashami martin jaggi 2023 landmark attention randomaccess infinite con arxiv preprint text transformers arxiv230516300 length guilherme penedo quentin malartic daniel hesslow ruxandra cojocaru alessandro cappelli hamza alobeidli baptiste pannier ebtesam almazrouei julien launay 2023 refinedweb dataset falcon llm outperforming curated corpora arxiv preprint web data web data only arxiv230601116 open bilingual pretrained model arxiv preprint arxiv221002414 susan zhang stephen roller naman goyal mikel artetxe moya chen shuohui chen christopher wan mona diab xian victoria lin al 2022 opt open pretrained transformer language models arxiv preprint arxiv220501068 ofir press noah smith mike lewis 2022 train short test long attention linear biases enables input length extrapolation international confer ence learning representations markus rabe charles staats 2021 selfattention arxiv preprint need memory arxiv211205682 colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu 2020 exploring limits transfer learning unified texttotext trans former journal machine learning research 21154855551 teven scao angela fan christopher akiki lie pavlick suzana ilic daniel hesslow roman castagn alexandra sasha luccioni franois yvon matthias gall al 2022 bloom 176b parameter openaccess multilingual language model arxiv preprint arxiv221105100 jianlin su 2021 scaling attention lens tropy invariance jianlin shengfeng pan ahmed murtadha wen yunfeng liu 2021 roformer hanced transformer rotary position embedding arxiv preprint arxiv210409864 mosaicml nlp team 2023 introducing mpt new standard opensource commercially usable llms httpswwwmosaicmlcomblogmpt 30b hugo touvron thibaut lavril gautier izacard xavier martinet marieanne lachaux timothe lacroix baptiste rozire naman goyal eric hambro faisal azhar al 2023 llama open effi cient foundation language models arxiv preprint arxiv230213971 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin 2017 attention need advances neural information processing systems 30 martin wilk ram gnanadesikan 1968 probabil ity plotting methods analysis analysis data biometrika 551117 shunyu yao binghui peng christos papadimitriou karthik narasimhan 2021 selfattention net works process bounded hierarchical languages proceedings 59th annual meeting association computational linguistics 11th international joint conference natural lan guage processing volume long papers pages 37703785 online association computational linguistics aohan zeng xiao liu zhengxiao zihan wang hanyu lai ming ding zhuoyi yang yifan wendi zheng xiao xia al 2022 glm130b figure line retrieval temperature analysis curves given proposition cross signs dots given algorithm logl 512 given yao al 2021 2021 appendix a1 realworld temperatures verify proposition remaining retrieval task plotting temperature curves figure empirically evaluate tr training length ex extrapolation length considering 0th layer a2 detailed expansion eq cid90 lel 22 elel 22ll2 22 cid90 cid90 e22 cid90 l22 22 l224 22 e222 a3 temperatures retrieval tasks report temperatures retrieval tasks model sizes given algorithm table a4 breakdown multidocument question answering report performance breakdown different number input documents table figure passkey retrieval temperature analysis curves given proposition cross signs dots given algorithm logl 512 given yao al 2021 2021 models flant5large pmax flant5xl pmax flant5xxl pmax topic topics retrieval tasks line lines passkey sentences 200 300 400 500 600 680 20k 30k 40k 50k 55k 085 08 075 075 075 085 08 08 075 075 075 085 080 080 075 075 07 06 055 05 05 065 055 055 05 05 05 06 055 05 05 05 08 075 07 065 065 08 075 075 07 070 07 085 08 075 075 075 07 055 055 05 05 06 055 055 05 05 05 07 065 06 06 06 085 08 075 075 075 08 08 075 075 075 075 085 08 08 075 075 075 065 06 055 055 065 06 06 055 055 055 065 06 055 055 05 table temperatures retrieval tasks search optimal temperature 10 095 09 05 multidocument question answering docs models docs docs flant5large 606 485 480 545 440 396 380 402 526 420 365 340 339 339 379 609 498 486 535 456 408 397 413 508 445 395 364 359 358 370 max 589 501 473 524 452 404 3800 400 476 411 352 335 322 333 342 ent 640 554 589 606 479 451 473 553 584 446 400 399 417 464 548 flant5xl 653 573 608 622 516 490 494 560 609 491 460 449 463 491 557 max ent 647 567 600 593 501 479 498 551 524 435 421 403 420 429 513 flant5xxl 651 610 646 611 539 524 547 624 589 491 481 475 489 531 612 662 618 632 628 559 544 556 596 604 525 510 502 513 535 591 max 673 621 613 632 561 541 543 576 610 534 508 503 507 519 557 ent table performance breakdown retrieval tasks numbers accuracy score 100 9 indicate position golden document contains answer question