attention alignment flexible positional embeddings improve transformer length extrapolation tachung chi carnegie mellon university tachungcandrew.cmu.edu tinghan fan independent researcher tinghanfalumni.princeton.edu alexander rudnicky carnegie mellon university aircs.cmu.edu abstract ideal lengthextrapolatable transformer lan guage model handle sequences longer training length long sequence finetuning. longcontext utilization pability highly relies flexible positional embedding design. investigating flex ibility existing large pretrained transformer language models, find family deserves closer look, positional embed dings capture rich flexible attention pat terns. however, suffers dispersed attention issue longer input sequence, flatter attention distribution. alleviate issue, propose attention alignment strategies temperature scaling. findings improve longcontext utilization capabil ity language modeling, retrieval, multidocument question answering finetuning, suggesting flexible sitional embedding design attention align ment long way transformer length extrapolation.1 introduction pretraining large transformer language models long sequences inherently expensive selfattentions quadratic complexity w.r.t sequence length vaswani al., 2017. help memoryefficient attention rabe staats, 2021 dao al., 2022, maximum supported input length current opensource pre trained transformer language models capped 4,096 tokens touvron al., 2023, limiting efficacy handling longcontext tasks. notable research topic aiming lift input length restriction length extrapo lation press al., 2022. ideally, length extrapolatable transformer language model trained short sequences perform equally 1httpsgithub.comchijames retrieval tasks line topic passkey 512 0.28 3.47 15k 0.12 6.63 512 0.27 3.47 15k 0.11 7.04 512 0.32 3.09 15k 0.24 5.97 criteria pmax table dispersed attention issue flant5 encoder. pmax averaged maximum proba bility averaged entropy. increasing sequence length 512 15k, observe larger entropy smaller maximum probability, implying flatter selfattention distribution. longer ones finetuning. possible carefully designed sitional embeddings press al., 2022 chi al., 2022, 2023. unfortunately, existing approaches tailored natural language modeling, task known strong recency bias, perform seemingly simple tasks passkey, topic, line retrieval htashami jaggi, 2023 al., 2023. circumvent recency bias, sift positional embeddings existing opensource large pretrained transformer language models table find flexible design, fam ily raffel al., 2020 comes attention. visualized figure flexibility t5s posi tional embeddings allows encourage recency bias head discourage head. however, free lunch suffers dispersed attention issue shown ble nutshell, attention distributions long input sequences tend flatter short input sequences. remedy, propose finetuningfree attention alignment strategies softmax temperature scaling yao al., 2021 su, 2021 mitigate dispersed attention sue maximum probability pmax entropy alignment. attentionalignmenttransformerlengthextrapolation validate effectiveness alignment models 2020 opt 2022 chatglm 2022 llama 2023 falcon 2023 pythia 2023 xgen 2023 bloom 2022 mpt 2023 pe. learned learned relative absolute rotary relative rotary relative rotary relative rotary relative rotary relative alibi relative alibi relative table opensource transformer language models positional embeddings. model equipped learnable relative positional embeddings, enable longcontext utilization capability. strategies tasks including language model ing, retrieval, multidocument question answer ing. provide theoretical analysis alignment strategies work hood investigating relation softmax tem perature data distribution. related work transformer embeddings positional transformerbased models rely positional embeddings encode positional information. summarize opensource large pretrained transformer language models positional embeddings table relative variants widely adopted better empirical performance al., 2021 possible lengthextrapolatable capability press al., 2022. work, special focus positional embeddings flexibility shown figure transformer length extrapolation existing research transformer length extrapolation task natural language mod eling press al., 2022 chi al., 2022, 2023. unfortunately, reported positive results carry task longcontext retrieval htashami jaggi, 2023 al., 2023. contrastive observation explained mod els short empirical receptive field chi al., 2023. short, strong decaying prior positional beddings prevents models accessing faraway tokens necessary retrieval tasks. work, improve flexible positional embed dings limitation. transformer position interpolation instead performing direct length extrapolation, line research conducts model finetuning long input sequences chen al., 2023, main focus identify efficient fine tuning scheme improves longcontext utiliza tion. positive results reported retrieval tasks al., 2023. however, argue finetuning incurs additional costs needs gpu resources perform long sequence fine tuning large models predefined target sequence length, imposes sequence length upper limit. proposed methods cir cumvent limitations. tasks retrieval transformers transformerbased approaches consist retriever reader overcome context length restriction guu al., 2020 lewis al., 2020 izacard grave, 2021 borgeaud al., 2022. retriever retrieves relevant text snippets huge database reader digests retrieved information generate correct output. proposed attention alignment strategy significantly increase input sequence length reader, allowing retrieved information participate decision process. smallscale retrieval problems, methods obviates need context segmentation external keyvalue store prior work mohtashami jaggi, 2023, serving elegant approach. softmax temperature scaling increase length extrapolation capability transformers, previous work yao al., 2021 su, 2021 scales temperature softmax logarithmically w.r.t sequence length ensure invariant entropy. entropy alignment strategy inspired line research adopt differ ent procedure outlined later algorithm inter estingly, experiment results 5.2 aligning entropy performs worse proposed maximum probability alignment strategy. longcontext retrieval tasks 3.1 retrieval suggested recent work mohtashami jaggi, 2023 al., 2023, task long context retrieval serves controllable bench mark measure transformer language model utilizes longcontext inputs. prominent characteristic retrieval tasks subset input interest, requiring model accu 1st attention head 27nd attention head figure visualization positional embeddings. plot figures bm,n, set 7500 vary value 15k. attention head flant5xl encoder learns set positional embeddings capture different attention bias. example, positional embeddings left figure encourage model focus nearby tokens. contrast, ones right figure let model focus remote tokens. rately pick necessary information. characteristic key information sit input, requiring model attend flexibly. finally, controllable aspect allows gradually increase input sequence length test models length extrapolation capability. 3.2 solve retrieval tasks transformer language models, necessary choose positional bedding design permits accurate flexible lengthextrapolatable attention. checking existing positional embeddings ble find family raffel al., 2020 fits needs. candidates, learnable abso lute positional embeddings vaswani al., 2017 zhang al., 2022 evaluated training length. alibi press al., 2022 tary al., 2021 recency bias, extrapolate easily finetuning. attention head, encoder maintains bucket learnable parameters assigns relative positional bias rpe bias bm,n as2 bm,n  16, bmin15, logmn8  bmin31, lognm8 log1288 log1288 8, 8, 8, position indices. bm,n added 2httpsgithub.comhuggingfacetransformers blobv4.33.2srctransformersmodelst5 modelingt5.pyl390 entry selfattention matrix. summation input temperature scaled softmax. plot learned rpe bias encoder figure tell attention heads encode rich attention patterns. example, head learns focus nearby tokens head learns ignore nearby tokens allow access faraway tokens. 3.3 dispersed attention issue encoder unfortunately, directly applying models trieval tasks yield perfect results. inspecting intermediate model states, find longer input sequence consists kens competing i.e., softmax sums attention, resulting dispersed tention issue. table longer input sequence, flatter selfattention distri bution. situation hopeless desired information attains higher attention weight remaining tokens. proposed solution let key information stand out. proposed methods natural solution dispersed attention issue described sharpen selfattention dis tribution, achieved reducing temperature extrapolation. set trapolation temperature ex sharpness training tr extrap olation ex roughly same. measurement sharpness, explore maxi algorithm attention alignment strategies require short sequence length ltr long sequence length lex. encoder align ment mode ensure softmax temperature function finds 0.5. outline procedure alignment strategies algorithm note proposed methods require model finetuning gradient updates. model family pretrained sequences length 512, set ltr 512 experiments paper. set temperature softmax operation perform operation operation softmax maximum probability append maxsoftmax entropy append hsoftmax end end end return avgs end function str1 finds1.0, ex 1.0, 0.95, 0.9, 0.5 sexex findsex, end return ex s.t. sexex str1 mum probability entropy distribution. words, proposed solution align maximum probability entropy training extrapolation distributions adjusting ex. concretely, let ith presoftmax logit vector encoder, ltr, lex sequence length. postsoftmax distri bution pi softmax li. maximum probability entropy pi max hi respectively. let maximum probability alignment strategy example. run ward pass compute averaged maximum probability temperature logit vec tors pmax cid80 max number logit vectors encoder layers, heads, length sequences. temperature training ex extrapolation, denote averaged maximum probability train ing ptr max1 extrapolation pex maxex. finally, align maximum probabil ity, adjust ex pex max1. practice, grid search ex 1.0 maxex ptr experiments 5.1 language modeling following previous work transformer length extrapolation, perform intrinsic evaluation language modeling press al., 2022 chi al., 2022, 2023. ideally, proposed methods alleviate perplexity explosion problem hap pens extrapolation. ble alignment strategies dramatically prove lower perplexity. want empha size perplexity primary focus attempt surpass previous work metric lower perplexity accu rately reflect longcontext utilization capability transformers practical tasks al., 2023. models sequence length 1024 2048 4096 8192 15000 52.7 56.0 50.0 63.4 44.3 43.8 t5largelm 35.9 40.1 pmax 34.7 45.5 45.2 45.5 40.2 43.9 45.6 54.6 t5xllm 28.3 pmax 30.2 36.0 31.6 41.7 30.4 36.8 38.4 53.3 t5xxllm 109 pmax 32.2 29.7 29.5 36.6 26.8 28.1 34.2 37.8 table language modeling performance. numbers perplexity. lower better. use adapted models experiment.3 5.2 longcontext retrieval inspired recently proposed retrieval tasks, evaluate proposed alignment strategies retrieval tasks. topic retrieval requires model retrieve topic long multitopic con versation al., 2023. line retrieval long series keyvalue pairs, model needs trieve value corresponding given key al., 2023. passkey retrieval hides passkey 3httpsgithub.comgoogleresearch texttotexttransfertransformerblobmain releasedcheckpoints.mdlmadaptedt511lm100k retrieval tasks models line, lines topic, topics 100 flant5large 100 pmax 100 100 100 100 100 flant5xl pmax 100 100 100 100 100 100 100 flant5xxl 100 100 100 pmax 100 100 100 100 100 100 100 100 100 longchat passkey, sentences 200 300 400 500 600 680 20k 30k 40k 50k 55k 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 avg. table performance retrieval tasks. numbers accuracy. score 100. longchat model corresponds longchat13b16k model al., 2023. llama13b touvron al., 2023 model finetuned sequences length 16k positional interpolation technique chen al., 2023. flant5xxl 11b parameters. maximum lengths tasks 14.5k 15k tokens. topic, topics flant5xl pmax 200 300 400 500 600 680 20k 30k 40k 50k 55k 0.8 0.75 0.7 0.65 0.65 0.8 0.75 0.75 0.7 0.70 0.7 0.85 0.8 0.75 0.75 0.75 0.7 0.55 0.55 0.5 0.5 0.6 0.55 0.55 0.5 0.5 0.5 0.7 0.65 0.6 0.6 0.6 retrieval tasks line, lines passkey, sentences table temperatures retrieval tasks. search optimal temperature 1.0, 0.95, 0.9, 0.5. long junk text snippet, model needs return passkey mohtashami jaggi, 2023. tasks formulated question swering format, use flant5 models leverage instructionfollowing capability. table retrieval perfor mance greatly boosted flant5 models equipped proposed attention alignment strategies. particular, maximum probability alignment strategy gives better results board. note longchat al., 2023, best baseline, finetuned llama tou vron al., 2023 long sequences length 16k proposed methods need fine tuning. baselines mpt team, 2023 chatglm2 al., 2022 perform worse longchat. refer al. 2023 details. present optimal temperature given algorithm table see, temperature decreases input quence length increases. temperatures model sizes highly similar ones presented table find table appendix conduct temperature analysis later 6. 5.3 multidocument question answering choose multidocument question answer ing task downstream task liu al., 2023. model input consists questions multiple documents extracted naturalques tions kwiatkowski al., 2019 documents golden doc contains ground truth answer. table model equipped proposed maximum probability alignment strategy, consistently outperforms original model model sizes number input documents. apart better task performance, lieve attention dispersed attention issue dis cussed help demystify lostinthe middle phenomenon liu al., 2023 task transformer models tend perform worse ground truth sits near middle input context. let recall relative positional embed ding head learned figure ground truth answer sits middle, long models flant5large pmax improvement flant5xl pmax improvement flant5xxl pmax improvement docs avg. 52.4 53.1 0.7 52.1 59.4 61.1 1.7 60.5 63.6 63.7 0.1 63.6 33.9 35.9 36.5 39.5 docs, golden doc different positions multidocument question answering docs avg. 43.3 44.2 0.9 43.2 51.2 53.6 2.4 52.4 56.9 57.7 0.8 57.1 37.9 42.0 34.0 33.9 52.6 37.0 35.8 36.4 44.5 50.8 0.9 1.5 3.0 2.4 2.0 1.9 1.8 34.2 33.3 33.5 41.1 47.6 54.8 46.4 39.9 44.6 58.4 60.9 55.7 49.1 44.9 49.1 2.5 4.5 6.0 5.0 4.6 2.7 0.9 42.9 51.3 40.3 52.4 53.1 61.2 47.5 58.9 59.1 60.4 53.5 50.2 2.1 1.5 3.4 2.9 2.7 2.4 0.4 55.7 51.9 61.0 35.2 40.0 46.0 42.1 48.1 51.0 32.2 41.7 46.3 43.5 49.1 52.5 42.0 48.9 51.3 50.7 50.3 53.4 50.8 avg. 38.7 40.0 1.3 36.7 46.5 50.3 3.8 44.9 52.4 54.0 1.6 53.4 table performance multidocument qa. numbers accuracy. score 100. improvement row represents absolute accuracy improvement flant5 model equipped proposed maximum probability alignment strategy. performance breakdown, refer table appendix contexts sides competing atten tion weight. hypothesis holds true, expect performance boost prominent cases answer sits near middle. reveal performance breakdown number input documents 30. provement row, cases achieve greater improvement. strategies perfect formance drops ground truth answer position 29. believe han dled case pretty recency bias learned attention heads, additional temperature scaling sharpens distribution aggressively. acknowledge tradeoff 8. 5.4 overall observation maximum probability alignment strategy reliable bestperforming method tasks settings, echoing discussion 3.1 data, subset input useful. maximum probability alignment strat egy captures characteristic naturally, outperforming entropy alignment strategy cares holistic distribution. theoretical analysis shed light underlying mechanisms alignment strategies, establish connection softmax temperature data distribution empirically verified assump tions. focus 0th layer closest input embeddings average logit vectors attention heads. note crude approximation algorithm analysis purposes transformer language model typically encompasses multiple layers, algorithm maximum probabil ity entropy individual logit vectors opposed averaged one. compute averaged logit vector, start input sequence length trans model attention heads specifically, encoder context, generate presoftmax logit vectors, length here, number layers focus 0th layer. logit vectors indi vidually sorted, subsequently calculate average sorted logit vectors, resulting averaged logit vector length assumption length averaged logit vector normally distributed, i.e., entry 2. assess averaged logit entries fol low gaussian distribution, employ plots, illustrated figure plot wilk gnanadesikan, 1968 graphical technique comparing probability distributions plot ting quantiles other. point corresponds quantile second dis tribution ycoordinate plotted quantile distribution xcoordinate. short sequences 512 long sequences 15k figure plots flant5xl. experiment short long sequences. red reference line yx. retrieval tasks line topic passkey 512 8.61 15k 8.80 512 8.71 15k 8.97 512 8.75 15k 8.85 criteria lmax table largest logit entry flant5xl. lmax largest logit entry averaged logit vector. consistent different input sequence lengths. distributions comparison similar, points plot roughly align identity line, case, aim determine degree gaussian havior averaged logit vector, linearity plot serves indicator closer points identity line, gaussian distribution. assumption largest logit entry aver aged logit vector training extrapolation max ltr lex max. verified table 6.1 maximum probability alignment let lmax largest value logit vector let temperature softmax function. probability largest entry pmax elmax eli cid80l softmax shiftinvariant, logit vector zeromean cid80 next, according assumption denominator softmax approximated cid88 eli eeli e22 implies pmax approximately pmax elmax le22 training stage, temperature ptr max max eltr ltre2 tr2 gives expression largest logit entry training stage ltr max log cid16 maxltre2 ptr tr2cid17 according assumption largest probability extrapolation stage simplified max eltr lexe2 ex2 pex max cid16 max elex lexe2 maxltre2 ptr lexe2 ex2 ex2 tr2cid171 free parameter extrapolation, adjust carry maximum probability alignment strategy. proposition assumption adjust temperature align maximum probability ptr max max pex log ltr log log lex log tr2 ex2 assuming solve quadratic equa tion pick larger root final solution. 6.2 entropy alignment entropy discrete probability computed softmax eli cid88 log eli log cid80 eli cid80 eli denominator soft max, approximated eq. hand, note cid80 lieli lelel. 2, elel approximated detailed derivation appendix elel cid90 lel 22 e222 thus, combining eq. entropy approximated log log le22 le22 set training stage, htr log ltr extrapolation, align entropy i.e., hex htr adjusting log lex hex htr log ltr free parameter extrapolation, adjust carry entropy alignment strategy. proposition assumption adjust temperature align entropy htr hex ex cid113 log lex ltr 6.3 realworld temperatures verify proposition topic retrieval task plotting temperature curves figure empirically evaluate tr training length ex extrapolation length considering 0th layer. find temperature curves retrieval tasks appendix proposed alignment strategies lower temperature input sequence length increases, entropy alignment strategy aggressively. believe leads ferior performance observed table figure topic retrieval temperature analysis. curves given proposition cross signs dots given algorithm logl 512 given yao al. 2021 2021. addition, real temperatures given gorithm higher given propositions. checking perlayer tention distributions, find 0th layer flatter distributions compared higher layers. cause propositions derived based 0th layer flatter distribution needs lower temperature correct, temperatures given tend lower ones given gorithm takes average temperatures layers. finally, previously proposed logdecaying rule loglex ltr yao al., 2021 su, 2021 motivated idea entropy invariance, analysis shows similar maximum probability alignment strategy proposi tion conclusion paper, find model family great potential comes transformer length extrapolation. propose maximum probabil ity entropy alignment strategies fix t5s dispersed attention issue model finetuning. conduct experiments natural language mod eling, retrieval tasks, multidocument question answering demonstrate effectiveness proposed methods. finally, present simpli fied theoretical analysis elucidate tem perature scaled achieve attention alignment. hope work inspire future length extrapolatable transformer designs. limitations base theoretical analysis simplified transformer language model, fur ther improved taking layers interactions account. addition, find different layers different degrees distribu tion flatness, leveraged future work perform perlayer attention alignment. nally, temperature scaling scheme sharpen distribution aggressively discussed multidocument question answering experi ments. drawback possibly improved designing finegrained attention align ment strategy. acknowledgment author acknowledges support boeing company 2019stupa259. references salesforce ai. 2023. long sequence modeling xgen llm trained input sequence length. stella biderman, hailey schoelkopf, quentin gregory anthony, herbie bradley, kyle obrien, eric hal lahan, mohammad aflah khan, shivanshu purohit, usvsn sai prashanth, edward raff, al. 2023. pythia suite analyzing large language mod international els training scaling. conference machine learning, pages 23972430. pmlr. sebastian borgeaud, arthur mensch, jordan hoff mann, trevor cai, eliza rutherford, katie milli can, george van den driessche, jeanbaptiste lespiau, bogdan damoc, aidan clark, al. 2022. improving language models retrieving tril lions tokens. international conference chine learning, pages 22062240. pmlr. shouyuan chen, sherman wong, liangjian chen, yuandong tian. 2023. extending context window large language models positional interpolation. arxiv preprint arxiv2306.15595. tachung chi, tinghan fan, peter ramadge, alexander rudnicky. 2022. kerple kernelized relative positional embedding length extrapola tion. advances neural information processing systems neurips, new orleans, usa. tachung chi, tinghan fan, alexander rudnicky, peter ramadge. 2023. dissecting transformer length extrapolation lens receptive field analysis. annual meeting association computational linguistics acl, toronto, canada. tri dao, dan fu, stefano ermon, atri rudra, christopher r. 2022. flashattention fast memoryefficient exact attention ioawareness. advances neural information processing systems, 351634416359. zhengxiao du, yujie qian, xiao liu, ming ding, jiezhong qiu, zhilin yang, jie tang. 2022. glm general language model pretraining autoregres sive blank infilling. proceedings 60th nual meeting association computational linguistics volume long papers, pages 320335. kelvin guu, kenton lee, zora tung, panupong pasu pat, mingwei chang. 2020. retrieval augmented language model pretraining. international confer ence machine learning, pages 39293938. pmlr. gautier izacard edouard grave. 2021. leveraging passage retrieval generative models open main question answering. proceedings 16th conference european chapter associ ation computational linguistics main volume, pages 874880, online. association computa tional linguistics. tom kwiatkowski, jennimaria palomaki, olivia red field, michael collins, ankur parikh, chris alberti, danielle epstein, illia polosukhin, jacob devlin, ken ton lee, al. 2019. natural questions benchmark question answering research. transactions association computational linguistics, 7453 466. patrick lewis, ethan perez, aleksandra piktus, fabio petroni, vladimir karpukhin, naman goyal, hein rich kttler, mike lewis, wentau yih, tim rock tschel, al. 2020. retrievalaugmented generation knowledgeintensive nlp tasks. advances neu ral information processing systems, 3394599474. dacheng li, rulin shao, anze xie, ying sheng, lian min zheng, joseph gonzalez, ion stoica, xuezhe ma, hao zhang. 2023. long opensource llms truly promise context length. nelson liu, kevin lin, john hewitt, ashwin paran jape, michele bevilacqua, fabio petroni, percy liang. 2023. middle lan guage models use long contexts. arxiv preprint arxiv2307.03172. lost amirkeivan mohtashami martin jaggi. 2023. landmark attention randomaccess infinite con arxiv preprint text transformers. arxiv2305.16300. length guilherme penedo, quentin malartic, daniel hesslow, ruxandra cojocaru, alessandro cappelli, hamza alobeidli, baptiste pannier, ebtesam almazrouei, julien launay. 2023. refinedweb dataset falcon llm outperforming curated corpora arxiv preprint web data, web data only. arxiv2306.01116. open bilingual pretrained model. arxiv preprint arxiv2210.02414. susan zhang, stephen roller, naman goyal, mikel artetxe, moya chen, shuohui chen, christopher wan, mona diab, xian li, victoria lin, al. 2022. opt open pretrained transformer language models. arxiv preprint arxiv2205.01068. ofir press, noah smith, mike lewis. 2022. train short, test long attention linear biases enables input length extrapolation. international confer ence learning representations. markus rabe charles staats. 2021. selfattention arxiv preprint need memory. arxiv2112.05682. colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, peter liu. 2020. exploring limits transfer learning unified texttotext trans former. journal machine learning research, 21154855551. teven scao, angela fan, christopher akiki, lie pavlick, suzana ilic, daniel hesslow, roman castagn, alexandra sasha luccioni, franois yvon, matthias gall, al. 2022. bloom 176b parameter openaccess multilingual language model. arxiv preprint arxiv2211.05100. jianlin su. 2021. scaling attention lens tropy invariance. jianlin su, lu, shengfeng pan, ahmed murtadha, wen, yunfeng liu. 2021. roformer hanced transformer rotary position embedding. arxiv preprint arxiv2104.09864. mosaicml nlp team. 2023. introducing mpt new standard opensource, commercially usable llms. httpswww.mosaicml.comblogmpt 30b. hugo touvron, thibaut lavril, gautier izacard, xavier martinet, marieanne lachaux, timothe lacroix, baptiste rozire, naman goyal, eric hambro, faisal azhar, al. 2023. llama open effi cient foundation language models. arxiv preprint arxiv2302.13971. ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan gomez, ukasz kaiser, illia polosukhin. 2017. attention need. advances neural information processing systems, 30. martin wilk ram gnanadesikan. 1968. probabil ity plotting methods analysis analysis data. biometrika, 551117. shunyu yao, binghui peng, christos papadimitriou, karthik narasimhan. 2021. selfattention net works process bounded hierarchical languages. proceedings 59th annual meeting association computational linguistics 11th international joint conference natural lan guage processing volume long papers, pages 37703785, online. association computational linguistics. aohan zeng, xiao liu, zhengxiao du, zihan wang, hanyu lai, ming ding, zhuoyi yang, yifan xu, wendi zheng, xiao xia, al. 2022. glm130b figure line retrieval temperature analysis. curves given proposition cross signs dots given algorithm logl 512 given yao al. 2021 2021. appendix a.1 realworld temperatures verify proposition remaining retrieval task plotting temperature curves figure empirically evaluate tr training length ex extrapolation length considering 0th layer. a.2 detailed expansion eq. cid90 lel 22 elel 22ll2 22 cid90 cid90 e22 cid90 l22 22 l224 22 e222 a.3 temperatures retrieval tasks report temperatures retrieval tasks model sizes given algorithm table a.4 breakdown multidocument question answering report performance breakdown different number input documents table figure passkey retrieval temperature analysis. curves given proposition cross signs dots given algorithm logl 512 given yao al. 2021 2021. models flant5large pmax flant5xl pmax flant5xxl pmax topic, topics retrieval tasks line, lines passkey, sentences 200 300 400 500 600 680 20k 30k 40k 50k 55k 0.85 0.8 0.75 0.75 0.75 0.85 0.8 0.8 0.75 0.75 0.75 0.85 0.80 0.80 0.75 0.75 0.7 0.6 0.55 0.5 0.5 0.65 0.55 0.55 0.5 0.5 0.5 0.6 0.55 0.5 0.5 0.5 0.8 0.75 0.7 0.65 0.65 0.8 0.75 0.75 0.7 0.70 0.7 0.85 0.8 0.75 0.75 0.75 0.7 0.55 0.55 0.5 0.5 0.6 0.55 0.55 0.5 0.5 0.5 0.7 0.65 0.6 0.6 0.6 0.85 0.8 0.75 0.75 0.75 0.8 0.8 0.75 0.75 0.75 0.75 0.85 0.8 0.8 0.75 0.75 0.75 0.65 0.6 0.55 0.55 0.65 0.6 0.6 0.55 0.55 0.55 0.65 0.6 0.55 0.55 0.5 table temperatures retrieval tasks. search optimal temperature 1.0, 0.95, 0.9, 0.5. multidocument question answering docs models docs docs flant5large 60.6 48.5 48.0 54.5 44.0 39.6 38.0 40.2 52.6 42.0 36.5 34.0 33.9 33.9 37.9 60.9 49.8 48.6 53.5 45.6 40.8 39.7 41.3 50.8 44.5 39.5 36.4 35.9 35.8 37.0 max. 58.9 50.1 47.3 52.4 45.2 40.4 38.00 40.0 47.6 41.1 35.2 33.5 32.2 33.3 34.2 ent. 64.0 55.4 58.9 60.6 47.9 45.1 47.3 55.3 58.4 44.6 40.0 39.9 41.7 46.4 54.8 flant5xl 65.3 57.3 60.8 62.2 51.6 49.0 49.4 56.0 60.9 49.1 46.0 44.9 46.3 49.1 55.7 max. ent. 64.7 56.7 60.0 59.3 50.1 47.9 49.8 55.1 52.4 43.5 42.1 40.3 42.0 42.9 51.3 flant5xxl 65.1 61.0 64.6 61.1 53.9 52.4 54.7 62.4 58.9 49.1 48.1 47.5 48.9 53.1 61.2 66.2 61.8 63.2 62.8 55.9 54.4 55.6 59.6 60.4 52.5 51.0 50.2 51.3 53.5 59.1 max. 67.3 62.1 61.3 63.2 56.1 54.1 54.3 57.6 61.0 53.4 50.8 50.3 50.7 51.9 55.7 ent. table performance breakdown retrieval tasks. numbers accuracy. score 100. 9... indicate position golden document contains answer question.